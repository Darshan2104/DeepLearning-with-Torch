{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "000b563f-7e6e-4b0b-a359-fc7b4613e6bf",
   "metadata": {},
   "source": [
    "# K Nearest Neighbour\n",
    "- It is supervised ML algorithm\n",
    "- We have X training data of size (X_train --> MxN) an y as lable (Mx1)\n",
    "- For any new data we need to identify K nearest neighboure from training data and give me max voted y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1275fb5-9081-4fec-aa39-7e327c0fdb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9497ff9-13b7-43d9-be56-c090e7e65cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN:\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "    # Predict method to classify a new data point\n",
    "    def predict(self, X_test):\n",
    "        predictions = [self._predict(x) for x in X_test]\n",
    "        return np.array(predictions)\n",
    "\n",
    "    @staticmethod\n",
    "    def _euclidean_distance(a, b):\n",
    "        return np.sqrt(np.sum((a-b)**2))\n",
    "\n",
    "    @staticmethod\n",
    "    def _cosine_similarity(a, b):\n",
    "        dot_product = np.dot(a, b)\n",
    "        norm_a = np.linalg.norm(a)\n",
    "        norm_b = np.linalg.norm(b)\n",
    "        return dot_product / (norm_a * norm_b)\n",
    "        \n",
    "    # Helper method to predict a single point\n",
    "    def _predict(self, x):\n",
    "        # Compute the Euclidean distance between x and all training points\n",
    "        distances = [self._euclidean_distance(x, x_train) for x_train in self.X_train]\n",
    "        \n",
    "        # Get the indices of the k nearest samples\n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "        \n",
    "        # Extract the labels of the k nearest samples\n",
    "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
    "        \n",
    "        # Majority vote, most common label\n",
    "        most_common = Counter(k_nearest_labels).most_common(1)\n",
    "        return most_common[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d7da053-734b-4728-8114-0f3c6f443f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [1 0]\n"
     ]
    }
   ],
   "source": [
    "# Sample training data\n",
    "X_train = np.array([[1, 2], [2, 3], [3, 4], [6, 7], [7, 8]])\n",
    "y_train = np.array([0, 0, 0, 1, 1])\n",
    "\n",
    "# New points to classify\n",
    "X_test = np.array([[7, 7], [5, 5]])\n",
    "\n",
    "# Create a KNN classifier instance with k=3\n",
    "knn = KNN(k=3)\n",
    "\n",
    "# Fit the model with training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict the classes for the new data points\n",
    "predictions = knn.predict(X_test)\n",
    "print(f\"Predictions: {predictions}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9025eae9-fe71-4fb8-b002-1683292e7a4a",
   "metadata": {},
   "source": [
    "# KNN Interview Questions\n",
    "\n",
    "### 1. What is K-Nearest Neighbors (KNN) algorithm?\n",
    "K-Nearest Neighbors is a supervised machine learning algorithm used for classification and regression tasks, which classifies a data point based on the majority class of its k-nearest neighbors.\n",
    "\n",
    "### 2. How does KNN work?\n",
    "- The algorithm stores the entire training dataset.\n",
    "- For a new data point, KNN calculates the distance between this point and all other points in the training dataset.\n",
    "- It selects the k-nearest neighbors (based on the chosen distance metric).\n",
    "- The majority class among these neighbors is assigned to the new data point.\n",
    "\n",
    "### 3. What are the distance metrics used in KNN?\n",
    "Some common distance metrics are:\n",
    "- Euclidean distance\n",
    "- Manhattan distance\n",
    "- Minkowski distance\n",
    "- Cosine similarity\n",
    "\n",
    "### 4. How do you choose the value of k in KNN?\n",
    "The value of `k` can be chosen using cross-validation. A small value of `k` may lead to overfitting, while a large value may lead to underfitting. Ideally, `k` should be odd for binary classification problems to avoid ties.\n",
    "\n",
    "### 5. What happens when `k=1` in KNN?\n",
    "When `k=1`, the algorithm assigns the class of the nearest neighbor. This may lead to overfitting since it makes predictions based on a single instance.\n",
    "\n",
    "### 6. How is KNN different from other classification algorithms?\n",
    "KNN is a **lazy learning algorithm**, meaning it does not learn a discriminative function from the training data but memorizes the training dataset. Other algorithms like decision trees or SVM are **eager learners** that try to generalize from the training data.\n",
    "\n",
    "### 7. What is the time complexity of KNN?\n",
    "- Training time complexity: O(1), since KNN doesn’t involve an explicit training phase.\n",
    "- Prediction time complexity: O(n \\* d), where `n` is the number of training samples and `d` is the number of dimensions. This is due to the distance computation between the test sample and every training sample.\n",
    "\n",
    "### 8. What are the advantages of using KNN?\n",
    "- Simple to implement and understand.\n",
    "- No training phase, which makes it well-suited for small datasets.\n",
    "- Can work well with multi-class classification.\n",
    "\n",
    "### 9. What are the disadvantages of using KNN?\n",
    "- KNN can be slow in prediction for large datasets since it requires calculating the distance to every training point.\n",
    "- It is memory-intensive as it stores the entire dataset.\n",
    "- Performance may degrade with noisy or high-dimensional data.\n",
    "- KNN can be sensitive to the choice of distance metric.\n",
    "\n",
    "### 10. How can you speed up KNN for large datasets?\n",
    "Some methods to improve KNN’s performance on large datasets include:\n",
    "- **Dimensionality reduction techniques** (e.g., PCA, t-SNE) to reduce the number of features.\n",
    "- **KD-Tree** or **Ball Tree** data structures to speed up the nearest neighbor search.\n",
    "- **Approximate nearest neighbor (ANN)** algorithms.\n",
    "- Parallelizing the distance calculations.\n",
    "\n",
    "### 11. Is KNN sensitive to outliers?\n",
    "Yes, KNN can be sensitive to outliers because it bases its classification on the nearest data points. Outliers can distort distance measurements and affect the classification.\n",
    "\n",
    "### 12. Can KNN be used for regression tasks?\n",
    "Yes, in KNN regression, the output is the average (or weighted average) of the k-nearest neighbors rather than the majority vote, as in classification.\n",
    "\n",
    "### 13. What are the applications of KNN in real-world scenarios?\n",
    "- Recommender systems (e.g., collaborative filtering).\n",
    "- Pattern recognition (e.g., handwriting or image classification).\n",
    "- Medical diagnosis (e.g., predicting diseases based on symptoms).\n",
    "- Anomaly detection.\n",
    "\n",
    "### 14. How can you handle missing data in KNN?\n",
    "Missing data can be handled by:\n",
    "- **Imputation**: Fill in missing values with the mean, median, or mode of the dataset.\n",
    "- **Ignoring samples with missing values**: Discard samples that have missing data.\n",
    "- Use a distance metric that can handle missing data.\n",
    "\n",
    "### 15. Can KNN handle categorical data?\n",
    "Yes, KNN can handle categorical data by encoding it. You can use methods like:\n",
    "- **Label encoding**: Convert categorical values into numerical labels.\n",
    "- **One-hot encoding**: Convert categories into binary vectors.\n",
    "Note: The choice of distance metric will also need to accommodate categorical features (e.g., Hamming distance).\n",
    "\n",
    "### 16. How does KNN handle imbalanced datasets?\n",
    "KNN can struggle with imbalanced datasets because the majority class can dominate the prediction. Techniques to handle this include:\n",
    "- **Oversampling the minority class** or **undersampling the majority class**.\n",
    "- Using **distance-weighted KNN**, where closer neighbors contribute more to the vote than farther ones.\n",
    "\n",
    "### 17. How can you prevent overfitting in KNN?\n",
    "Overfitting in KNN can occur when `k` is too small. To prevent it:\n",
    "- Increase the value of `k` to reduce sensitivity to individual noisy points.\n",
    "- Use **distance-weighted KNN** to give closer points more influence.\n",
    "\n",
    "### 18. Can KNN be used for time series data?\n",
    "KNN can be used for time series classification, but it requires appropriate preprocessing, such as using dynamic time warping (DTW) for distance measurement rather than Euclidean distance.\n",
    "\n",
    "### 19. Why is KNN called a non-parametric algorithm?\n",
    "KNN is called a non-parametric algorithm because it does not make any assumptions about the underlying distribution of the data. It doesn’t learn a fixed number of parameters during the training phase like other parametric algorithms (e.g., linear regression).\n",
    "\n",
    "### 20. What are weighted KNN and its benefits?\n",
    "In **weighted KNN**, each neighbor is assigned a weight proportional to its distance from the test point. Closer neighbors have more influence on the prediction. This can improve accuracy, especially when the decision boundary is not linear.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682337b8-2f75-4b21-bd04-4334257ecc50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
