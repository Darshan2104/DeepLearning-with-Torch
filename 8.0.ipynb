{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq with LSTM \n",
    "1. tokenize input data and Do a train test split\n",
    "2. Build Vocab on training data\n",
    "3. Build Encoder module\n",
    "4. Build Decoder module\n",
    "5. Seq2Seq module (which uses both encoder and decoder)\n",
    "\n",
    "- Reference : \n",
    "    - https://arxiv.org/pdf/1409.3215.pdf\n",
    "    - https://github.com/pytorch/text/blob/release/0.9/examples/legacy_tutorial/migration_tutorial.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# from torchtext.legacy import Field, BucketIterator\n",
    "\n",
    "# !pip install torch==1.8.0 torchtext==0.9.0\n",
    "from torchtext.datasets import IMDB,IWSLT2016\n",
    "from torchtext.legacy import data\n",
    "from torchtext.legacy import datasets\n",
    "\n",
    "import numpy as np\n",
    "import spacy\n",
    "import random\n",
    "\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter # to print to tensorboard"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save_checkPoints and Load_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# ................... How to use .................\\n# checkpoint = {\"state_dict\":model.state_dict(),\\n#               \"optimizer\": optimizer.state_dict()\\n#               }\\n\\n# save_checkpoint(checkpoint)\\n\\n# load_checkpoint(model, optimizer, checkpoint)\\n\\n'"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def save_checkpoint(state, file_name = \"mycheckpoint.pth.ptar\"):\n",
    "    print(\"Saving checkpoints --->\\n\")\n",
    "    torch.save(state,file_name)\n",
    "    \n",
    "\n",
    "def load_checkpoint(model, optimizer, checkpoint):\n",
    "    print(\"Loading checkpoint --->\\n\")\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# ................... How to use .................\n",
    "# checkpoint = {\"state_dict\":model.state_dict(),\n",
    "#               \"optimizer\": optimizer.state_dict()\n",
    "#               }\n",
    "\n",
    "# save_checkpoint(checkpoint)\n",
    "\n",
    "# load_checkpoint(model, optimizer, checkpoint)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre-processing\n",
    "1. Train/validate/test split: generate train/validate/test data set if they are available\n",
    "2. Tokenization: break a raw text string sentence into a list of words\n",
    "3. Vocab: define a \"contract\" from tokens to indexes\n",
    "4. Numericalize: convert a list of tokens to the corresponding indexes\n",
    "5. Batch: generate batches of data samples and add padding if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Internal error: confirm_token was not found in Google drive link.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[392], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_iter, validate_iter, test_iter \u001b[39m=\u001b[39m IWSLT2016(split\u001b[39m=\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mvalid\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m'\u001b[39;49m))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchtext/data/datasets_utils.py:166\u001b[0m, in \u001b[0;36m_wrap_split_argument_with_fn.<locals>.new_fn\u001b[0;34m(root, split, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m result \u001b[39m=\u001b[39m []\n\u001b[1;32m    165\u001b[0m \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m _check_default_set(split, splits, fn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m):\n\u001b[0;32m--> 166\u001b[0m     result\u001b[39m.\u001b[39mappend(fn(root, item, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n\u001b[1;32m    167\u001b[0m \u001b[39mreturn\u001b[39;00m _wrap_datasets(\u001b[39mtuple\u001b[39m(result), split)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchtext/datasets/iwslt2016.py:230\u001b[0m, in \u001b[0;36mIWSLT2016\u001b[0;34m(root, split, language_pair, valid_set, test_set)\u001b[0m\n\u001b[1;32m    227\u001b[0m src_test, tgt_test \u001b[39m=\u001b[39m test_filenames\n\u001b[1;32m    229\u001b[0m extracted_files \u001b[39m=\u001b[39m []  \u001b[39m# list of paths to the extracted files\u001b[39;00m\n\u001b[0;32m--> 230\u001b[0m dataset_tar \u001b[39m=\u001b[39m download_from_url(SUPPORTED_DATASETS[\u001b[39m'\u001b[39;49m\u001b[39mURL\u001b[39;49m\u001b[39m'\u001b[39;49m], root\u001b[39m=\u001b[39;49mroot, hash_value\u001b[39m=\u001b[39;49mSUPPORTED_DATASETS[\u001b[39m'\u001b[39;49m\u001b[39mMD5\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    231\u001b[0m                                 path\u001b[39m=\u001b[39;49mos\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(root, SUPPORTED_DATASETS[\u001b[39m'\u001b[39;49m\u001b[39m_PATH\u001b[39;49m\u001b[39m'\u001b[39;49m]), hash_type\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmd5\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    232\u001b[0m extracted_dataset_tar \u001b[39m=\u001b[39m extract_archive(dataset_tar)\n\u001b[1;32m    233\u001b[0m \u001b[39m# IWSLT dataset's url downloads a multilingual tgz.\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[39m# We need to take an extra step to pick out the specific language pair from it.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchtext/utils.py:140\u001b[0m, in \u001b[0;36mdownload_from_url\u001b[0;34m(url, path, root, overwrite, hash_value, hash_type)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    136\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mGoogle drive link \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m is currently unavailable, because the quota was exceeded.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    137\u001b[0m                 url\n\u001b[1;32m    138\u001b[0m             ))\n\u001b[1;32m    139\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInternal error: confirm_token was not found in Google drive link.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    142\u001b[0m \u001b[39mif\u001b[39;00m confirm_token:\n\u001b[1;32m    143\u001b[0m     url \u001b[39m=\u001b[39m url \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m&confirm=\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m confirm_token\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Internal error: confirm_token was not found in Google drive link."
     ]
    }
   ],
   "source": [
    "train_iter, validate_iter, test_iter = IWSLT2016(split=('train', 'valid', 'test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_iter, test_iter = IMDB(split=('train', 'test'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load data and do train-validate-test split\n",
    "- Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "('Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.', 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?', ['Saint Bernadette Soubirous'], [515])\n"
     ]
    }
   ],
   "source": [
    "for a,b in enumerate(train_iter):\n",
    "    print(a)\n",
    "    print(b)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.', 'What is in front of the Notre Dame Main Building?', ['a copper statue of Christ'], [188])\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(train_iter)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "# train_iter = IMDB(split='train')\n",
    "counter = Counter()\n",
    "for (label, line) in train_iter:\n",
    "    # print(label)\n",
    "    # print(line)\n",
    "    counter.update(tokenizer(line))\n",
    "    \n",
    "vocab = Vocab(counter, min_freq=10, specials=('<unk>', '<pad>', '<BOS>', '<EOS>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(vocab.stoi['<unk>'])\n",
    "print(vocab.stoi['<pad>'])\n",
    "print(vocab.stoi['<BOS>'])\n",
    "print(vocab.stoi['<EOS>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the new vocab is 20437\n",
      "output of the text_transform: [2, 134, 12, 43, 467, 3]\n",
      "result :  0\n"
     ]
    }
   ],
   "source": [
    "# tokenizer(\"here is an example\")\n",
    "print(\"The length of the new vocab is\", len(vocab))\n",
    "\n",
    "'''\n",
    "idx = vocab.stoi\n",
    "print(idx)\n",
    "\n",
    "new_stoi = vocab.stoi\n",
    "print(\"The index of '' is\", new_stoi['the'])\n",
    "'''\n",
    "\n",
    "text_transform = lambda text: [vocab['<BOS>']] + [vocab[token] for token in tokenizer(text)] + [vocab['<EOS>']]\n",
    "# pos = 1 and neg = 0\n",
    "label_transform = lambda lable: 1 if lable == 'pos' else 0\n",
    "\n",
    "\n",
    "print(\"output of the text_transform:\", text_transform(\"here is an example\"))\n",
    "print(\"result : \", label_transform(\"neg\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------- OLD LEGEACY CODE ---------------------------------------\n",
    "\n",
    "# spacy_german = spacy.load('de')\n",
    "# spacy_english = spacy.load('en')\n",
    "\n",
    "# def tokenizer_german(text):\n",
    "#     return [token.text for token in spacy_german.tokenizer(text)]\n",
    "\n",
    "# def tokenizer_english(text):\n",
    "#     return [token.text for token in spacy_english.tokenizer(text)]\n",
    "\n",
    "# # <sos> : start of sentence\n",
    "# german = Field(tokenizer = tokenizer_german, lower = True, init_token = '<sos>', eos_token = '<eos>')\n",
    "# english = Field(tokenizer = tokenizer_english, lower = True, init_token = '<sos>', eos_token = '<eos>')\n",
    "\n",
    "# train_data, validate_data, Test_data = Multi30k.splits(exts = ('.de','.en'), fields =(german, english))\n",
    "# # Build Vocab\n",
    "# german.build_vocab(train_data, max_size = 10000, min_freq = 2)\n",
    "# english.build_vocab(train_data, max_size = 10000, min_freq = 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_size : size of vocabulary\n",
    "# embedding_side : each is mapped to some d-dimenssional space\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,input_size, embedding_size, hidden_size, number_of_layers,dropout):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.number_of_layers = number_of_layers\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.embedding = nn.Embedding(input_size,embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, number_of_layers, dropout = dropout)\n",
    "    \n",
    "# x: vector of indices for a given input sentence\n",
    "    def forward(self, x):\n",
    "    # x(shape) : sequence_length x N(batch size) \n",
    "        \n",
    "        embedding = self.dropout(emb = self.embedding(x))\n",
    "        # embedding_shape : (sequence_length, N, embedding_size)\n",
    "        # for each word we can have mapping to some d-dimenssion space ; d is size of embedding vector\n",
    "        output, (hiddden, cell) = self.rnn(embedding)\n",
    "        return hiddden, cell"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, number_of_layers, dropout):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.number_of_layer = number_of_layers\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, number_of_layers, dropout=dropout)\n",
    "        self.fully_connected = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        # Shape of x : N but we want (1,N); we will get one token as output and we want to pass it as next input so....\n",
    "        x = x.unsqueeze(0)\n",
    "        \n",
    "        embedding = self.dropout(self.embedding(x))                 # embedding shape : (1, N, embedding_size)\n",
    "        output, (hidden,cell) = self.rnn(embedding, (hidden,cell))  # shape of output : (1, N, hidden_size)\n",
    "        # predictions = self.fully_connected(output)                  # shape of output : (1, N, length_of_vocab)\n",
    "        output = self.softmax(self.fully_connected(output[0]))\n",
    "        \n",
    "        # prediction = predictions.squeeze(0)\n",
    "        # shape : (N, length_of_vocab)\n",
    "        return output, hidden, cell\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, source_sentence, target_setence,teacher_force_ration=0.5):\n",
    "        batch_size = source_sentence[1]\n",
    "        target_len = target_setence[0]\n",
    "        target_vocab_size = len(vocab)\n",
    "\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size)\n",
    "\n",
    "        hidden, cell = self.encoder(source_sentence)\n",
    "        \n",
    "        # grab start token\n",
    "        x = target_setence[0]\n",
    "\n",
    "        for t in range(1, target_len):\n",
    "            output, hidden, cell = self.decoder(x,hidden,cell)\n",
    "            outputs[t] = output\n",
    "\n",
    "            # output shape: (N, target_vocab_size)\n",
    "\n",
    "            best_guess = output.argmax(1)\n",
    "\n",
    "            x = target_setence[t] if random.random() < teacher_force_ration else best_guess\n",
    "\n",
    "        return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, test_iter = IMDB(split=('train', 'test'))\n",
    "train_list = list(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "('neg', '\"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn\\'t matter what one\\'s political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn\\'t true. I\\'ve seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don\\'t exist. The same goes for those crappy cable shows: schlongs swinging in the breeze but not a clitoris in sight. And those pretentious indie movies like The Brown Bunny, in which we\\'re treated to the site of Vincent Gallo\\'s throbbing johnson, but not a trace of pink visible on Chloe Sevigny. Before crying (or implying) \"double-standard\" in matters of nudity, the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women: there are no genitals on display when actresses appears nude, and the same cannot be said for a man. In fact, you generally won\\'t see female genitals in an American film in anything short of porn or explicit erotica. This alleged double-standard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of women\\'s bodies.')\n"
     ]
    }
   ],
   "source": [
    "print(len(train_list))\n",
    "print((train_list[1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create batch and each batch has similer size of input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import random\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# process the raw text data and add padding to dynamically match the longest sentence in a batch\n",
    "def collate_batch(batch_of_data):\n",
    "    label_list, text_list = [], []\n",
    "    for _label, _text in batch_of_data:\n",
    "        label_list.append(label_transform(_label))\n",
    "        processed_text = torch.tensor(text_transform(_text))\n",
    "        text_list.append(processed_text)\n",
    "    return torch.tensor(label_list), pad_sequence(text_list, padding_value=3.0)\n",
    "\n",
    "# # batch_size = 8  # A batch size of 8\n",
    "\n",
    "# def batch_sampler():\n",
    "#     indices = [(i, len(tokenizer(s[1]))) for i, s in enumerate(train_list)]\n",
    "#     # print(\"indices :\\n\",indices)\n",
    "#     random.shuffle(indices)\n",
    "#     pooled_indices = []\n",
    "#     # create pool of indices with similar lengths \n",
    "#     for i in range(0, len(indices), batch_size * 100):\n",
    "#         pooled_indices.extend(sorted(indices[i:i + batch_size * 100], key=lambda x: x[1]))\n",
    "\n",
    "#     # print(\"pooled_indices :\\n\",pooled_indices)\n",
    "#     # p(pooled_indices)\n",
    "#     pooled_indices = [x[0] for x in pooled_indices]\n",
    "\n",
    "#     # yield indices for current batch\n",
    "#     for i in range(0, len(pooled_indices), batch_size):\n",
    "#         yield pooled_indices[i:i + batch_size]\n",
    "\n",
    "\n",
    "# train_Batch_iter = DataLoader(train_list, batch_sampler=batch_sampler(),collate_fn=collate_batch)\n",
    "train_Batch_iter = DataLoader(train_list, batch_size=batch_size, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for l,d in train_Batch_iter:\n",
    "#     print(l[0])\n",
    "#     print(d[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter = 0\n",
    "# for a,b in enumerate(train_Batch_iter):\n",
    "#     if counter == 2:\n",
    "#         break\n",
    "#     print(a)\n",
    "#     print(b[0])\n",
    "#     print(b[1])\n",
    "#     counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "train_iter, test_iter = IMDB(split=('train', 'test'))\n",
    "train_list = list(train_iter)\n",
    "test_list = list(test_iter)\n",
    "\n",
    "print(train_list.__len__())\n",
    "print(test_list.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training hyperparameters\n",
    "num_epochs = 5\n",
    "learning_rate = 0.01\n",
    "batch_size = 64\n",
    "\n",
    "# model hyperparameters\n",
    "load_model = False\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_size_encoder = len(vocab)     # len(german.vocab)\n",
    "input_size_decoder = len(vocab)     # len(english.vocab)\n",
    "\n",
    "output_size = len(vocab)            # len(english.vocab)\n",
    "encoder_embedding_size = 300\n",
    "decoder_embedding_size = 300\n",
    "hidden_size = 1024\n",
    "number_of_layer = 2\n",
    "encoder_dropout = 0.5\n",
    "decoder_dropout = 0.5\n",
    "\n",
    "# TensortBoard\n",
    "writer = SummaryWriter(f\"runs/loss_plot\")\n",
    "\n",
    "step = 0\n",
    "# train_iterator, validation_iterator, test_iterator = BucketIterator.split(\n",
    "#     batch_size = batch_size,\n",
    "#     sort_within_batch = True,\n",
    "#     sort_key = lambda x:len(x.src),\n",
    "#     device = device)\n",
    "\n",
    "# train_iter, test_iter = IMDB(split=('train', 'test'))\n",
    "\n",
    "# train_iterator = DataLoader(list(train_iter),batch_sampler=batch_sampler(),collate_fn=collate_batch)\n",
    "# test_iterator = DataLoader(list(test_iter),batch_sampler=batch_sampler(),collate_fn=collate_batch)\n",
    "\n",
    "# input_size, embedding_size, hidden_size, number_of_layers,dropout\n",
    "encoder_network = Encoder(input_size_encoder, encoder_embedding_size, hidden_size, number_of_layer, encoder_dropout).to(device)\n",
    "decoder_network = Decoder(input_size_encoder, decoder_embedding_size, hidden_size, output_size, number_of_layer, decoder_dropout).to(device)\n",
    "\n",
    "model = Seq2Seq(encoder_network, decoder_network).to(device)\n",
    "optimizer = optim.Adam(model.parameters(),lr=learning_rate)\n",
    "\n",
    "pad_idx = vocab.stoi['<pad>']\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(train_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/5]\n",
      "Saving checkpoints --->\n",
      "\n",
      "pme tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Here you go tensor([[   2,    2,    2,  ...,    2,    2,    2],\n",
      "        [  15,   15,   53,  ...,   15,   15,   16],\n",
      "        [1573,  248,   72,  ...,  484,   99,   23],\n",
      "        ...,\n",
      "        [   3,    3,    3,  ...,    3,    3,    3],\n",
      "        [   3,    3,    3,  ...,    3,    3,    3],\n",
      "        [   3,    3,    3,  ...,    3,    3,    3]])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "zeros() received an invalid combination of arguments - got (str, Tensor, int), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[377], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m inp_data \u001b[39m=\u001b[39m text\n\u001b[1;32m     18\u001b[0m target \u001b[39m=\u001b[39m label\n\u001b[0;32m---> 20\u001b[0m output \u001b[39m=\u001b[39m model(inp_data, target)\n\u001b[1;32m     21\u001b[0m \u001b[39m# output_shape : (trg_len, batch_size, output_dim)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m output \u001b[39m=\u001b[39m output[\u001b[39m1\u001b[39m:]\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,output\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    890\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "Cell \u001b[0;32mIn[367], line 12\u001b[0m, in \u001b[0;36mSeq2Seq.forward\u001b[0;34m(self, source_sentence, target_setence, teacher_force_ration)\u001b[0m\n\u001b[1;32m      9\u001b[0m target_len \u001b[39m=\u001b[39m target_setence[\u001b[39m0\u001b[39m]\n\u001b[1;32m     10\u001b[0m target_vocab_size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(vocab)\n\u001b[0;32m---> 12\u001b[0m outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mzeros(target_len, batch_size, target_vocab_size)\n\u001b[1;32m     14\u001b[0m hidden, cell \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(source_sentence)\n\u001b[1;32m     16\u001b[0m \u001b[39m# grab start token\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: zeros() received an invalid combination of arguments - got (str, Tensor, int), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "# have to implement it seprately\n",
    "# from utils import translate_sentence, blue, save_checkpoint, load_checkpoint\n",
    "if load_model:\n",
    "    load_checkpoint(model, optimizer, torch.load('mycheckpoint.pth.ptar'),model,optimizer)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}]\")\n",
    "\n",
    "    checkpoint = {'static_dict' : model.state_dict(), 'optimizer':optimizer.state_dict()}\n",
    "    save_checkpoint(checkpoint)\n",
    "\n",
    "    for batch_idx, (lable, text) in enumerate(train_Batch_iter):\n",
    "        # inp_data = batch.text.to(device)\n",
    "        # target = batch.labal.to(device)\n",
    "        inp_data = text\n",
    "        target = label\n",
    "\n",
    "        output = model(inp_data, target)\n",
    "        # output_shape : (trg_len, batch_size, output_dim)\n",
    "\n",
    "        output = output[1:].reshape(-1,output.shape[2])\n",
    "        target = target[1:].reshape(-1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output,target)\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),max_norm=1)\n",
    "        optimizer.step()\n",
    "\n",
    "        writer.add_scalar('Train loss',loss, global_step=step)\n",
    "        step+=1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
